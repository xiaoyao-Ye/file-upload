# file-upload

上传大文件

1. 普通文件上传(前端上传图片/文件, 服务器做持久化存储后返回链接) [考点: 图片文件信息, 文件大小校验, 字符串和十六进制ascii码]
  1.1 优化项: 进度条, 成功后弹窗提示
  1.2 可扩展: 拖拽上传, 粘贴上传
  1.3 安全问题: 上传文件类型限制(光靠文件名限制不安全, 可重命名越过限制, 最好使用文件二进制数据信息判断类型), 上传文件大小限制
2. 大文件分片上传 [考点: Blob.slice 切片, 断点续传, 秒传, 怎么判断文件是否存在]
  2.1 切片上传 -
  2.2 断点续传(询问后端是否部分切片已存在, 已存在的前端不需要再次上传, 全部上传完毕后后端把文件拼在一起还原文件) -
  2.3 秒传(文件已经上传过, 服务器已经有了, 无需再次上传, 通过文件hash值判断) -
    2.3.1 文件太大, 无法一次性读取到内存中, 无法计算hash值, 怎么处理?
    2.3.2 通过 web-worker 计算文件hash值 -
    2.3.3 通过 时间切片计算md5值 -
    2.3.4 通过 抽样哈希(比较偏门) -
  2.4 并发数控制(浏览器并发6, 控制为4, 预留2) + 错误重试(比如网络波动, 重试3次) (本身就是一个面试题)
  2.5 慢启动(先上传一个块, 后续根据上传块的时间去调整块的大小)
3. 思考:
  3.1 碎片清理
  3.2 文件碎片存储在多个机器上
  3.3 文件碎片备份
  3.4 兼容性更好的 requestIdleCallback
  3.5 并发 + 慢启动
  3.6 抽样hash + 全量hash双重判断
  3.7 websocket 推送
  3.8 cdn...

## TODO

- [ ] 用户体验优化
  - [x] 按钮loading状态
  - [ ] 暂停/取消按钮
- [x] 基本上传
- [ ] 拖拽上传
- [ ] 粘贴上传
- [x] 上传类型限制
- [ ] 上传类型限制(根据二进制信息判断)
- [x] 上传大小限制
- [x] 上传成功后返回静态文件地址url
- [x] 上传进度条
- [x] 上传成功后提示
- [x] 分片上传
- [x] 控制上传并发数量(除了解决性能问题还能解决部分请求超时的问题)
- [x] 断点续传
- [x] 秒传
- [ ] web-worker 计算哈希
- [ ] 时间切片计算md5
- [ ] 抽样哈希
- [ ] 慢启动

- [x] 开静态资源服务器
- [x] 保存 chunk
- [x] 合并 chunk
- [ ] 清理临时文件
- [ ] chunk目录和文件目录分开

- [x] 服务器部分使用 nest.js
- [x] 前端部分使用 element-plus

## question

- 上传到服务器, 有临时文件, 需要定时清理.
- axios 设置有超时时间, 当上传文件比较大的时候, 后端合并文件可能需要比较久(大概以G为单位会出现?), 或者是chunk数量特别多 都会导致 merge 请求超时 axios 自动取消请求, 无法合并.
- 分片数量过多(无论大小)时, 浏览器 + node runtime = cpu 100%, 计算哈希?有一定时间卡顿
- 单文件并且只能切一个chunk时, 使用同一个上传接口? 如果是的话, 上传后还需要调用merge接口不是很合理, 单个chunk的让后端直接merge?
- 上传文件名称要是出现xx-xx-xx的格式, 可能会影响排序时的切割.上传的文件可能同名? 一律使用hash?
- 有时候上传的内容会异常, 怀疑是merge那里的问题(已存在再次上传合并异常概率高?)

## other

启动项目:

> 启动对应的项目需要先去对应的项目安装 node_modules

```bash
pnpm install
# 前端
pnpm run dev
# 服务端
pnpm run start
```
